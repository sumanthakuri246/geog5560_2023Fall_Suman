{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb34fb2-9ba5-4a0c-ba28-d3ae0528d649",
   "metadata": {
    "id": "2cb34fb2-9ba5-4a0c-ba28-d3ae0528d649"
   },
   "source": [
    "#Land Use Land Cover Map of Denton County using Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c8b4d-79ed-4d46-afb0-dd92053eca44",
   "metadata": {
    "id": "4c6c8b4d-79ed-4d46-afb0-dd92053eca44"
   },
   "source": [
    "#**Abstract**\n",
    "This study explores the dynamic interaction between land use and land cover (LULC) in Denton County, Texas, employing Google Earth Engine (GEE) and Jupyter Notebook for image classification. Utilizing the National Agriculture Imagery Program (NAIP) data, the project focuses on creating a detailed Land Use Land Cover map. The integration of Python libraries, including ee and geemap, facilitates cloud-based computation, enhancing efficiency in handling large datasets. The Support Vector Machine (SVM) algorithm is employed for image classification, demonstrating its effectiveness in delineating diverse land cover types. Denton County, within the Dallas-Fort Worth metropolitan area, serves as an ideal study area, given its varied landscape, including residential areas, open spaces, forests, and water bodies. The study showcases the power of remote sensing techniques, emphasizing their utility for monitoring and mapping land use land cover changes in a rapidly evolving region. The combination of GEE and Jupyter Notebook proves instrumental, providing a comprehensive and efficient approach for researchers and practitioners engaged in planetary-scale environmental data analysis. Despite the project's achievements, the classification results presented certain limitations, including misclassifications and feature omissions. To address these challenges, the exploration of more advanced classification methods, such as neural networks and integrated approaches, is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda5dfc-d531-416d-8382-80fa1ee118b4",
   "metadata": {
    "id": "7cda5dfc-d531-416d-8382-80fa1ee118b4"
   },
   "source": [
    "#**Introduction**\n",
    "A given spatial region's landscape is defined by its land use and land cover (LULC) combinations. These two terminologies stood differently in their definition. Land cover to the physical and biological entities covering the surface of the earth including vegetation, water bodies, and bare land whereas land use refers to the use of land by humans for any purpose which includes residential area, agricultural land, and industrial area (Liping et al., 2018; Fonji & Taff, 2014). Global LULC change has a substantial effect on ecosystems, biodiversity, biological cycles, and the environment (Ellis & Pontius, 2006). Furthermore, sustainable development and land use land cover change (LULCC) are closely related concepts. At the local or global levels, land cover change (LCC) plays a critical role in climate change. The change in landscape is studied by means of analyzing the land use and land cover change. Remote sensing technologies are the most common approach for determining land use and land cover change (Fonji & Taff, 2014).\n",
    "\n",
    "Google Earth Engien (GEEe is a cloud-based platform for planetary-scale environmental data analysis .With a dedicated Code Editor for development, it provides a vast database of remote sensing data and offers analysis in Python and JavaScript via the Earth Engine API. By utilizing a range of datasets like Landsat, Sentinel, and National Agriculture Program (NAIP) users can carry out a variety of analysis, such as picture categorization, time series evaluations, and mapping land cover. Interactive exploration is made possible by visualization tools, and shared projects are made easier by collaborative capabilities. Google Earth Engine GEE() is a useful tool for scientists and researchers doing planetary-scale analyses since it has applications in climate studies, agriculture, environmental monitoring, and other fields.\n",
    "\n",
    "Jupyter Notebook is an open-source, web-based interactive computing environment that facilitates the creation and sharing of documents containing live code, equations, visualizations, and narrative text. Python, R, and Julia are just a few of the computer languages that Jupyter supports. Its versatility makes it a top option for academics, data scientists, and instructors. Iterative and exploratory coding processes are made possible by the ability for users to develop and run code in cells. Markdown cells go beyond code to allow formatted text, graphics, and mathematical expressions to be incorporated, allowing story and computational aspects to be seamlessly integrated inside a single document. Jupyter Notebooks are a powerful tool for data analysis, machine learning experiments, and scientific research due to their real-time visuals and instant feedback. Additionally, the ability to convert notebooks to several formats promotes cooperation and the sharing of knowledge.\n",
    "\n",
    "With this study, I used the image inventory of Google Earth Engine (GEE) and jupyter notebook to do land use land cover classification of Denton County. The objective of this project was to prepare of Land Use Land Cover map of Denton county using National Agriculture Imagery Program (NAIP). Along with, I wanted to explore python libraries and methods essential for effective image classification. Though this classification can be done in software like ArcGIS and QGIS, but this program led to cloud based computation so that large data download for the classification is not required while working on inventory of Google Earth Engine (GEE). For this purpose, I used the python libraries ee which allows to interact with the Earth  Engine platform using python and geemap for doing geospatial analysis such as image classification Support Vector Machine (SVM) algorithm was used for image classification. Support Vector Machine (SVM) algorithms are powerful tools in the field of image classification. SVMs belong to the category of supervised machine learning algorithms, specifically designed for both classification and regression tas (Torabzadeh et al., 2014)..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f90c61-22a2-4ffd-87f3-a8bfc619def1",
   "metadata": {
    "id": "d6f90c61-22a2-4ffd-87f3-a8bfc619def1"
   },
   "source": [
    "#**Study Area**\n",
    "Denton County is a county located in the U.S. state of Texas. Denton County is situated in the northern part of the state of Texas, within the Dallas-Fort Worth metropolitan area. It is part of the Dallas-Fort Worth-Arlington Metropolitan Statistical Area (DFW Metroplex). With the two public universities, University of North Texas and Texas Women’s University, the city has attracted people from different states and countries to live here. With the typical landcover type such as residential area, open spaces, forest, and water bodies, Denton is appropriate for land cover studies. As the Denton is growing, it became crucial to address the land cover change within several categories.  Ground based survey for determining land use land cover change is a costly and time-consuming approach. Use of remote sensing techniques is an effective and rapid way to monitor and map land use land cover change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc128d-e92e-4e1e-84ac-74ff5a5a09ba",
   "metadata": {
    "id": "aafc128d-e92e-4e1e-84ac-74ff5a5a09ba"
   },
   "source": [
    "#**Data Required**\n",
    "The study was focused on determining the land use land cover map of Denton County for year 2022 using National Agriculture Imagery Program (NAIP) images. The data is available for public. The NAIP orthophotos were acquired during the year 2022 agricultural growing season (April 2022 – December 2022) with a mix of leaf-on/leaf-off conditions throughout Texas by National Agriculture Imagery Program (Texas NAIP imagery .2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d27eaf-c030-4464-96d1-8c783d2b6651",
   "metadata": {
    "id": "f2d27eaf-c030-4464-96d1-8c783d2b6651"
   },
   "source": [
    "#**Project Design**\n",
    " This project was divided into two sections, first section consist of collecting the required orthophoto and training sample, and classification of the the image based on the training sample. This was done using Earth Engine (ee) and Google Earth Engine (geemap) python libraries. Different methods of ee and geemap were used in this section such as classification, validation, and interactive presentation of map. In second section, the classified image was provided with appropriate color, map layout and map components to prepare map . For this section, raterio and matplotlib along with their methods were used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bf968c-54b7-472a-92c4-e27cac7401d8",
   "metadata": {
    "id": "53bf968c-54b7-472a-92c4-e27cac7401d8"
   },
   "outputs": [],
   "source": [
    "'''Following python libraries should be installed and imported to do image classification and map layout.\n",
    "The program check if these libraries in the device and import the libraries otherwise installed and import them'''\n",
    "\n",
    "try:\n",
    "    import ee\n",
    "    import geemap\n",
    "    import rasterio as rio\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    from matplotlib.patches import Patch\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "except:\n",
    "    !pip install earthengine-api\n",
    "    !pip install geemap\n",
    "    !pip install rasterio\n",
    "    !pip install matplotlib\n",
    "    !pip install matplotlib-scalebar\n",
    "\n",
    "    import ee\n",
    "    import geemap\n",
    "    import rasterio as rio\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    from matplotlib.patches import Patch\n",
    "    from matplotlib_scalebar.scalebar import ScaleBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80d918d-e33a-45b6-bcfb-a13b1f485e93",
   "metadata": {
    "id": "a80d918d-e33a-45b6-bcfb-a13b1f485e93",
    "outputId": "c3ef7df4-3737-42e5-9207-aa3496eebf7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate to your Earth Engine account\n",
    "#ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72e6b4d-728e-4ecc-80db-578dd0180a68",
   "metadata": {
    "id": "c72e6b4d-728e-4ecc-80db-578dd0180a68",
    "outputId": "b4201329-5c6f-4942-89e6-4dad0c6faac9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Earth Engine API\n",
    "ee.Initialize()\n",
    "# Load the FeatureCollections GEE assests and inventory\n",
    "dataset = ee.ImageCollection('USDA/NAIP/DOQQ').filter(ee.Filter.date('2022-01-01', '2022-10-31')).median()\n",
    "builtup = ee.FeatureCollection(\"projects/project-python-231204/assets/builtup\")\n",
    "farmland = ee.FeatureCollection(\"projects/project-python-231204/assets/farmland\")\n",
    "opensp = ee.FeatureCollection(\"projects/project-python-231204/assets/open_space\")\n",
    "trees = ee.FeatureCollection(\"projects/project-python-231204/assets/trees\")\n",
    "water = ee.FeatureCollection(\"projects/project-python-231204/assets/water\")\n",
    "denton = ee.FeatureCollection(\"projects/project-python-231204/assets/denton\")\n",
    "clipped_img = dataset.clip(denton)\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Merge the FeatureCollections to create a composite class\n",
    "classNames = builtup.merge(farmland).merge(opensp).merge(trees).merge(water)\n",
    "\n",
    "# Add a random column to split data into training and validation sets\n",
    "classNames = classNames.randomColumn()\n",
    "\n",
    "# Set the split ratio\n",
    "split = 0.7  # Roughly 70% training, 30% testing.\n",
    "\n",
    "# Filter data for training and validation\n",
    "train_arg1 = classNames.filter(ee.Filter.lt('random', split))\n",
    "valid_arg1 = classNames.filter(ee.Filter.gte('random', split))\n",
    "\n",
    "# Sample training data from the image\n",
    "training = clipped_img.sampleRegions(\n",
    "    collection=train_arg1,\n",
    "    properties=['value'],\n",
    "    scale=30\n",
    ")\n",
    "\n",
    "# Train a classifier using training data\n",
    "classifier_denton = ee.Classifier.libsvm().train(training, 'value')\n",
    "\n",
    "# Classify the image\n",
    "classified_denton = clipped_img.classify(classifier_denton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b901092c-35d2-4321-a0e6-1f9537b17f5e",
   "metadata": {
    "id": "b901092c-35d2-4321-a0e6-1f9537b17f5e",
    "outputId": "ccb1b677-ce1d-452c-d43a-a8283f1e5456"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Matrix: [[0, 0, 0, 0, 0, 0], [0, 24, 1, 1, 0, 4], [0, 1, 59, 3, 0, 2], [0, 0, 7, 44, 21, 9], [0, 0, 3, 8, 80, 1], [0, 0, 2, 4, 2, 56]]\n",
      "Overall Accuracy: 0.7921686746987951\n"
     ]
    }
   ],
   "source": [
    "# Validate the classifier using the validation data\n",
    "validation = clipped_img.sampleRegions(\n",
    "    collection=valid_arg1,\n",
    "    properties=['value'],\n",
    "    scale=30\n",
    ")\n",
    "# Perform the validation\n",
    "validated = validation.classify(classifier_denton)\n",
    "\n",
    "# Get the error matrix comparing the validation data and the classified result\n",
    "errorMatrix = validated.errorMatrix('value', 'classification')\n",
    "\n",
    "# Print the error matrix\n",
    "print('Error Matrix:', errorMatrix.getInfo())\n",
    "\n",
    "# Print overall accuracy\n",
    "accuracy = errorMatrix.accuracy()\n",
    "overall_accuracy_value = accuracy.getInfo()\n",
    "print('Overall Accuracy:', overall_accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c380507a-070a-4a8b-973b-3ddb977dc1b9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "47e7bc1c038b4121829ae2d4858caa0b"
     ]
    },
    "id": "c380507a-070a-4a8b-973b-3ddb977dc1b9",
    "outputId": "ec8ea394-f87a-4f91-c733-8226ad0c2d08",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bd8fefb2fa40e99632f5d2aa2d0f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[33.28, -97.16], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=SearchDataG…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customize the color in of the classified image for interactive map\n",
    "Map.addLayer(classified_denton, {'min': 0, 'max': 5, 'palette': ['#000000', '#0000FF', '#006400', '#FFFF00', '#00FFFF', '#ff0000']}, 'Classification Denton')\n",
    "Map.setCenter(-97.16, 33.28, 10)\n",
    "\n",
    "#Add legend on interactive map\n",
    "legend_dict = {\n",
    "    'Built up': '#ff0000',\n",
    "    'Farmland': '#00FFFF',\n",
    "    'Open Space': '#FFFF00',\n",
    "    'Water': '#0000FF',\n",
    "    'Trees': '#006400',\n",
    "}\n",
    "# Display the map\n",
    "Map.add_legend(title=\"Land Cover Classification\", legend_dict=legend_dict)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860c7a67-9684-4eac-893c-8adfa9a61be7",
   "metadata": {
    "id": "860c7a67-9684-4eac-893c-8adfa9a61be7",
    "outputId": "04965a62-41d7-4fc8-fc67-2e9a57487d31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Export the map to google drive in a 'export' folder, it might take time. You can track the progress from tasks in google earth engine'''\n",
    "geemap.ee_export_image_to_drive(classified_denton, description='lulc', folder='export', region=denton.geometry(), scale=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad5cc0b-4f60-4d4b-8989-b1987e81c492",
   "metadata": {
    "id": "5ad5cc0b-4f60-4d4b-8989-b1987e81c492",
    "outputId": "1af46675-a253-420d-9ed3-bbcde78ca45b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "                    \n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "                \n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "                    \n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "lulc.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mrasterio\\\\_base.pyx:69\u001b[0m, in \u001b[0;36mrasterio._base.get_dataset_driver\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\\\_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: lulc.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m''' The classified image that is uploaded in the drive should be downloaded and placed in the same folder of jupyter notebook'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Open the raster file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rio\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlulc.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[0;32m      4\u001b[0m     arr \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Custom colors for each class\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\rasterio\\env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\rasterio\\__init__.py:319\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m DatasetReader(path, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 319\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[0;32m    320\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    321\u001b[0m     )\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m driver:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\rasterio\\io.py:299\u001b[0m, in \u001b[0;36mget_writer_for_path\u001b[1;34m(path, driver)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the writer class appropriate for the existing dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m driver:\n\u001b[1;32m--> 299\u001b[0m     driver \u001b[38;5;241m=\u001b[39m get_dataset_driver(path)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_writer_for_driver(driver)\n",
      "File \u001b[1;32mrasterio\\\\_base.pyx:74\u001b[0m, in \u001b[0;36mrasterio._base.get_dataset_driver\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: lulc.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "''' The classified image that is uploaded in the drive should be downloaded and placed in the same folder of jupyter notebook'''\n",
    "# Open the raster file\n",
    "with rio.open(\"lulc.tif\", 'r+') as src:\n",
    "    arr = src.read(1)\n",
    "\n",
    "    # Custom colors for each class\n",
    "    colors = ['#ffffff', '#0000FF', '#006400', '#FFFF00', '#00FFFF', '#ff0000']  # Add your custom colors here\n",
    "\n",
    "    # Create a colormap from the custom colors\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    # Plot your data using matplotlib\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(arr, cmap=cmap)\n",
    "\n",
    "    # Create a legend with custom labels and colors\n",
    "    legend_labels = [\"Water\", \"Trees\", \"Open Space\", \"Farmland\", \"Built up\"]\n",
    "    legend_patches = [Patch(color=color, label=label) for color, label in zip(colors[1:], legend_labels)]  # Exclude the background color\n",
    "\n",
    "    # Add a title to the legend\n",
    "    legend_title = \"Land Cover Classes\"\n",
    "    legend = ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', title=legend_title)\n",
    "\n",
    "    # Customize the legend title appearance\n",
    "    plt.setp(legend.get_title(), fontsize='12')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    #plt.title(\"Land Use Land Cover Classification of Denton County, Texas\")\n",
    "    plt.title(\"Land Use Land Cover Classification of Denton County, Texas\", fontsize=14, fontweight='bold', fontstyle='italic')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add a north arrow to the top right corner rotated by 120 degrees clockwise\n",
    "    x, y, arrow_length = 1.03, 1.03, 0.1\n",
    "    ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "    arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "    ha='center', va='center', fontsize=20,\n",
    "    xycoords=ax.transAxes)\n",
    "\n",
    "    # # Add a scale bar shifted more to the right\n",
    "    scalebar = ScaleBar(1, location='lower right', border_pad=-0.8, scale_loc=\"right\",pad=0.05)  # Increase the pad value\n",
    "    ax.add_artist(scalebar)\n",
    "\n",
    "    # Save the figure to a local file\n",
    "    plt.savefig(\"land_cover_map.png\", bbox_inches='tight', pad_inches=0.1, dpi = 700)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74634-d6e1-4af9-ac66-5b177189e5ae",
   "metadata": {
    "id": "52c74634-d6e1-4af9-ac66-5b177189e5ae"
   },
   "source": [
    "#**Limitations**\n",
    "The classification process relies on an aerial image, and consequently, it hinges on the distinctive spectral signatures exhibited by various features. However, features that share similar spectral properties may be prone to misclassification, potentially leading to an inaccurate representation of the landscape. Furthermore, the spatial resolution of the image is set at 60 cm. Consequently, certain features may not be distinguishable at this resolution, resulting in potential omissions on the classified map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68bdb2-524d-4deb-a339-89c9113da8c6",
   "metadata": {
    "id": "0a68bdb2-524d-4deb-a339-89c9113da8c6"
   },
   "source": [
    "#**Conclusion**\n",
    "In summary, the Land Use Land Cover (LULC) map for Denton County in 2022 was generated utilizing NAIP imagery and training sample points. The project leveraged the image inventory from Google Earth Engine (GEE) and the geemap Python library for geospatial visualization and image classification. Additionally, matplotlib and rasterio Python libraries were employed for creating the map laout.\n",
    "However, the classification results exhibit certain limitations, leading to misclassifications and omissions of features. To address these challenges, more advanced classification methods, such as neural networks and integrated approaches, could be explor (Prasai et al., 2021; Yan et al., 2015)ed. As a potential avenue for future work, implementing multiple data sources, including LiDAR data and aerial imagery, in conjunction with neural networks, may enhance the accuracy of the classification process. This could contribute to a more refined and comprehensive understanding of the landscape features in Denton County."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b42b91-8bc3-4715-9521-49a5057baddf",
   "metadata": {
    "id": "34b42b91-8bc3-4715-9521-49a5057baddf"
   },
   "source": [
    "#**References**\n",
    "Ellis, E., & Pontius, R. (2006). Land use and land cover change. Remote Sensing of Environment, 158, 295-310. https://doi.org/10.1016/j.rsase.2006.130045\n",
    "\n",
    "Fonji, S. F., & Taff, G. N. (2014). Using satellite data to monitor land-use land-cover change in north-eastern latvia. SpringerPlus, 3(1), 61. https://doi.org/10.1186/2193-1801-3-61\n",
    "\n",
    "Liping, C., Yujun, S., & Saeed, S. (2018). Monitoring and predicting land use and land cover changes using remote sensing and GIS techniques—A case study of a hilly area, jiangle, china. PLoS ONE, 13(7), e0200493. https://doi.org/10.1371/journal.pone.0200493\n",
    "\n",
    "Prasai, R., Schwertner, T. W., Mainali, K., Mathewson, H., Kafley, H., Thapa, S., Adhikari, D., Medley, P., & Drake, J. (2021). Application of google earth engine python API and NAIP imagery for land use and land cover classification: A case study in florida, USA. Ecological Informatics, 66, 101474. https://doi.org/10.1016/j.ecoinf.2021.101474\n",
    "\n",
    "Texas NAIP imagery . (2020, https://data.tnris.org/collection/?c=aa5183ca-a1bd-4b5f-9b63-4ba48d01b83d.\n",
    "\n",
    "Torabzadeh, H., Morsdorf, F., & Schaepman, M. E. (2014). Fusion of imaging spectroscopy and airborne laser scanning data for characterization of forest ecosystems – A review. ISPRS Journal of Photogrammetry and Remote Sensing, 97, 25-35. https://doi.org/10.1016/j.isprsjprs.2014.08.001\n",
    "\n",
    "United states geological survey (USGS). www.usgs.gov. https://www.usgs.gov/landsat-missions/landsat-1\n",
    "\n",
    "Yan, W. Y., Shaker, A., & El-Ashmawy, N. (2015). Urban land cover classification using airborne LiDAR data: A review. Remote Sensing of Environment, 158, 295-310. https://doi.org/10.1016/j.rse.2014.11.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec5983-cc32-4424-bd08-2474233192a4",
   "metadata": {
    "id": "32ec5983-cc32-4424-bd08-2474233192a4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
